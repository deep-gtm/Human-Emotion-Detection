{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MaDx_C6s0XIe",
        "VsFcSvtb4ij4",
        "w4CVgC2P0fo_",
        "zlAUcyVR2xIc",
        "jeY04x0AQc4r",
        "WnwMoq8n3c5g",
        "WIzhT8519on0",
        "-WierCRPj-4E",
        "qKRmY-i_pcGK",
        "clHxD0MCpei-",
        "2QbuaoZPCuOO",
        "kwC95Ah3HBUi",
        "sqgFjyxCNwiZ",
        "Kz6UYRbrN1B0",
        "FZvbHxmRdJvt",
        "ArrcH8iwllkE",
        "RTkfagwLgDcR",
        "oWcZuPIOhj4V",
        "rftSrpZDhPS0",
        "A3IZkhSQEZOb",
        "FryVTvmshThI",
        "siYv0UTPrMzD",
        "nuOR44oDvr7C",
        "vlUbatI1wQGi",
        "1JYG1hQYIj06",
        "5wv0IMO5I12a",
        "dL7szUfXI6wO",
        "Td5hZkHTJCYs",
        "Bzc8y-cxIm1I",
        "al_0ArqFwNCD",
        "NeXr1uz4mb9V",
        "qSjBoRuMddwF",
        "TxH7e8ZAvUi-",
        "IXR2epowbN78",
        "41wfNutsmNAs",
        "Vc1P9tFhrj8T",
        "NlbPoniMr1nv",
        "qP2DW0FJnSa3",
        "RptuCozqnnVZ"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTS"
      ],
      "metadata": {
        "id": "MaDx_C6s0XIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf # For tensorflow\n",
        "import numpy as np # For mathematical computations\n",
        "import matplotlib.pyplot as plt # For plotting and Visualization\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.layers import Input, Layer, Resizing, Rescaling, InputLayer, Conv2D, BatchNormalization, MaxPooling2D, Dropout, Flatten, Dense, RandomRotation, RandomFlip, RandomContrast, ReLU, Add, GlobalAveragePooling2D, Permute\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.regularizers import L2\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy, TopKCategoricalAccuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
        "import cv2\n"
      ],
      "metadata": {
        "id": "cztynt5e0Uci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# wandb INSTALLATION"
      ],
      "metadata": {
        "id": "VsFcSvtb4ij4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "EHkKVFfA4oVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "metadata": {
        "id": "OvcQxAkc4uB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "pIBSb8_Z41RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"Human-Emotion-Detection\", entity=\"ishu9t2\")"
      ],
      "metadata": {
        "id": "4J5IRGMc46Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.config = {\n",
        "    \"BATCH_SIZE\":32,\n",
        "    \"IM_SIZE\": 224,\n",
        "    \"LEARNING_RATE\": 5e-5,\n",
        "    \"N_EPOCHS\": 20,\n",
        "    \"DROPOUT_RATE\": 0.0,\n",
        "    \"REGULARIZATION_RATE\": 0.0,\n",
        "    \"N_FILTERS\": 6,\n",
        "    \"KERNEL_SIZE\": 3,\n",
        "    \"N_STRIDES\": 1,\n",
        "    \"POOL_SIZE\": 2,\n",
        "    \"N_DENSE_1\": 1024,\n",
        "    \"N_DENSE_2\": 128,\n",
        "    \"NUM_CLASSES\": 3,\n",
        "    \"PATCH_SIZE\": 16,\n",
        "    \"PROJ_DIM\": 768,\n",
        "    \"CLASS_NAMES\": [\"angry\", \"happy\", \"sad\"],\n",
        "}\n",
        "CONFIGURATION = wandb.config"
      ],
      "metadata": {
        "id": "q_Y-Gpl95Qj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KAGGLE"
      ],
      "metadata": {
        "id": "w4CVgC2P0fo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle #installing kaggle"
      ],
      "metadata": {
        "id": "wTK8sYKO0V6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "nC1ORqmt0nBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json #changing permission to read and execute"
      ],
      "metadata": {
        "id": "McbTIsE604bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA LOADING"
      ],
      "metadata": {
        "id": "zlAUcyVR2xIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d muhammadhananasghar/human-emotions-datasethes #downloading dataset from kaggle (command from kaggle website)"
      ],
      "metadata": {
        "id": "kMvb1G6Q1Br9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "Sb_0YV7TNZ5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/human-emotions-datasethes.zip\" -d \"/content/dataset/\" # Unzipping dataset"
      ],
      "metadata": {
        "id": "COhJf7NV1Ozv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONFIGURATION / CONSTANTS"
      ],
      "metadata": {
        "id": "jeY04x0AQc4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DIR = \"/content/dataset/Emotions Dataset/Emotions Dataset/train\"\n",
        "TEST_DIR = \"/content/dataset/Emotions Dataset/Emotions Dataset/test\"\n",
        "VAL_DIR = \"/content/dataset/Emotions Dataset/Emotions Dataset/test\"\n",
        "\n",
        "CONFIGURATION = {\n",
        "    \"IM_SIZE\": 224,\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"LEARNING_RATE\": 0.001,\n",
        "    \"N_EPOCHS\": 20,\n",
        "    \"DROPOUT_RATE\": 0.0,\n",
        "    \"REGULARIZATION_RATE\": 0.0,\n",
        "    \"N_FILTERS\": 6,\n",
        "    \"KERNEL_SIZE\": 3,\n",
        "    \"N_STRIDES\": 1,\n",
        "    \"POOL_SIZE\": 2,\n",
        "    \"N_DENSE_1\": 1024,\n",
        "    \"N_DENSE_2\": 128,\n",
        "    \"N_DENSE_3\": 3,\n",
        "    \"NUM_CLASSES\": 3,\n",
        "    \"CLASS_NAMES\": [\"angry\", \"happy\", \"sad\"]\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "Chx7Pg145zh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET CREATION"
      ],
      "metadata": {
        "id": "WnwMoq8n3c5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    labels='inferred',  # Automatically infer class labels from subdirectories\n",
        "    label_mode='categorical',  # Specify the label mode (e.g., categorical, binary)\n",
        "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],  # Number of samples per batch\n",
        "    image_size=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),  # Target image size\n",
        "    shuffle=True,  # Shuffle the dataset\n",
        "    seed=123,  # Random seed for shuffling\n",
        ")"
      ],
      "metadata": {
        "id": "aI_--hxk1u0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    VAL_DIR,\n",
        "    labels='inferred',  # Automatically infer class labels from subdirectories\n",
        "    label_mode='categorical',  # Specify the label mode (e.g., categorical, binary)\n",
        "    batch_size=32,#CONFIGURATION[\"BATCH_SIZE\"],  # Number of samples per batch\n",
        "    image_size=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),  # Target image size\n",
        "    shuffle=True,  # Shuffle the dataset\n",
        "    seed=123,  # Random seed for shuffling\n",
        ")"
      ],
      "metadata": {
        "id": "rYqo0UsA7WV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in val_dataset.take(1):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "Za6yT8ft7rU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA VISUALIZATION"
      ],
      "metadata": {
        "id": "WIzhT8519on0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (14, 14))\n",
        "for images, labels in train_dataset.take(1):\n",
        "  for i in range(16):\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    plt.imshow(images[i]/255)\n",
        "    plt.title(train_dataset.class_names[tf.argmax(labels[i]).numpy()])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "lalr2sy472o5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_dataset.class_names)"
      ],
      "metadata": {
        "id": "EKFURteQBSMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uHHGL80yCKjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA AUGMENTATION"
      ],
      "metadata": {
        "id": "-WierCRPj-4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASIC AUGMENTATION"
      ],
      "metadata": {
        "id": "qKRmY-i_pcGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augment_layers = tf.keras.Sequential([\n",
        "    RandomRotation(factor=(-0.025, 0.025)),\n",
        "    RandomFlip(mode=\"horizontal\"),\n",
        "    RandomContrast(factor=0.1)\n",
        "])\n",
        "\n",
        "def augment_layer(image, label):\n",
        "  return augment_layers(image, training=True), label"
      ],
      "metadata": {
        "id": "N4vmSn89kDG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUT MIX AUGMENTATION"
      ],
      "metadata": {
        "id": "clHxD0MCpei-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\n",
        "    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n",
        "    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n",
        "    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n",
        "\n",
        "IMG_SIZE = CONFIGURATION[\"IM_SIZE\"]\n",
        "@tf.function\n",
        "def get_box(lambda_value):\n",
        "    cut_rat = tf.math.sqrt(1.0 - lambda_value)\n",
        "\n",
        "    cut_w = IMG_SIZE * cut_rat  # rw\n",
        "    cut_w = tf.cast(cut_w, tf.int32)\n",
        "\n",
        "    cut_h = IMG_SIZE * cut_rat  # rh\n",
        "    cut_h = tf.cast(cut_h, tf.int32)\n",
        "\n",
        "    cut_x = tf.random.uniform((1,), minval=0, maxval=IMG_SIZE, dtype=tf.int32)  # rx\n",
        "    cut_y = tf.random.uniform((1,), minval=0, maxval=IMG_SIZE, dtype=tf.int32)  # ry\n",
        "\n",
        "    boundaryx1 = tf.clip_by_value(cut_x[0] - cut_w // 2, 0, IMG_SIZE)\n",
        "    boundaryy1 = tf.clip_by_value(cut_y[0] - cut_h // 2, 0, IMG_SIZE)\n",
        "    bbx2 = tf.clip_by_value(cut_x[0] + cut_w // 2, 0, IMG_SIZE)\n",
        "    bby2 = tf.clip_by_value(cut_y[0] + cut_h // 2, 0, IMG_SIZE)\n",
        "\n",
        "    target_h = bby2 - boundaryy1\n",
        "    if target_h == 0:\n",
        "        target_h += 1\n",
        "\n",
        "    target_w = bbx2 - boundaryx1\n",
        "    if target_w == 0:\n",
        "        target_w += 1\n",
        "\n",
        "    return boundaryx1, boundaryy1, target_h, target_w\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def cutmix(train_ds_one, train_ds_two):\n",
        "    (image1, label1), (image2, label2) = train_ds_one, train_ds_two\n",
        "\n",
        "    alpha = [0.25]\n",
        "    beta = [0.25]\n",
        "\n",
        "    # Get a sample from the Beta distribution\n",
        "    lambda_value = sample_beta_distribution(1, alpha, beta)\n",
        "\n",
        "    # Define Lambda\n",
        "    lambda_value = lambda_value[0][0]\n",
        "\n",
        "    # Get the bounding box offsets, heights and widths\n",
        "    boundaryx1, boundaryy1, target_h, target_w = get_box(lambda_value)\n",
        "\n",
        "    # Get a patch from the second image (`image2`)\n",
        "    crop2 = tf.image.crop_to_bounding_box(\n",
        "        image2, boundaryy1, boundaryx1, target_h, target_w\n",
        "    )\n",
        "    # Pad the `image2` patch (`crop2`) with the same offset\n",
        "    image2 = tf.image.pad_to_bounding_box(\n",
        "        crop2, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE\n",
        "    )\n",
        "    # Get a patch from the first image (`image1`)\n",
        "    crop1 = tf.image.crop_to_bounding_box(\n",
        "        image1, boundaryy1, boundaryx1, target_h, target_w\n",
        "    )\n",
        "    # Pad the `image1` patch (`crop1`) with the same offset\n",
        "    img1 = tf.image.pad_to_bounding_box(\n",
        "        crop1, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE\n",
        "    )\n",
        "\n",
        "    # Modify the first image by subtracting the patch from `image1`\n",
        "    # (before applying the `image2` patch)\n",
        "    image1 = image1 - img1\n",
        "    # Add the modified `image1` and `image2`  together to get the CutMix image\n",
        "    image = image1 + image2\n",
        "\n",
        "    # Adjust Lambda in accordance to the pixel ration\n",
        "    lambda_value = 1 - (target_w * target_h) / (IMG_SIZE * IMG_SIZE)\n",
        "    lambda_value = tf.cast(lambda_value, tf.float32)\n",
        "\n",
        "    # Combine the labels of both images\n",
        "    label = lambda_value * label1 + (1 - lambda_value) * label2\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "SPXA9U3Spihi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET PREPERATION"
      ],
      "metadata": {
        "id": "2QbuaoZPCuOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAIN DATASET"
      ],
      "metadata": {
        "id": "eUbqr2jcz8fR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Without Augmentation\n",
        "training_dataset = (\n",
        "    train_dataset\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "aeEIrpq_zq0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Augmentation\n",
        "training_dataset = (\n",
        "    train_dataset\n",
        "    .map(augment_layer, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "ZAuOsiKwCw2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cutmix Augmentation + Basic\n",
        "\n",
        "train_dataset_1 = train_dataset.map(augment_layer, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset_2 = train_dataset.map(augment_layer, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "mixed_dataset = tf.data.Dataset.zip((train_dataset_1, train_dataset_2))\n",
        "\n",
        "training_dataset = (\n",
        "    mixed_dataset\n",
        "    .map(cutmix, num_parallel_calls = tf.data.AUTOTUNE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "ylrAIWv_zkyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VALIDATION DATASET"
      ],
      "metadata": {
        "id": "kXnthrnRz_zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = (\n",
        "    val_dataset\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "cWwT7bC7FZ-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODELLING"
      ],
      "metadata": {
        "id": "kwC95Ah3HBUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LENET MODEL"
      ],
      "metadata": {
        "id": "sqgFjyxCNwiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_model = tf.keras.Sequential([\n",
        "    # Input Layer, as images have different shapes therefore None, None, 3\n",
        "    InputLayer(input_shape=(None, None, 3)),\n",
        "\n",
        "    # Resing & Rescaling to make each image same dimensions and normalising there value\n",
        "    Resizing(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
        "    Rescaling(1./255),\n",
        "\n",
        "    # Convolution Layers for extracting features from images\n",
        "    # Convolution layer 1\n",
        "    Conv2D(filters=CONFIGURATION[\"N_FILTERS\"], kernel_size=CONFIGURATION[\"KERNEL_SIZE\"], strides=CONFIGURATION[\"N_STRIDES\"], padding=\"valid\", activation=\"relu\", kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "\n",
        "    # Batch normaliation, to have zero meand unit variance for each mini batch and normalzing inputs to each layer\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # Downsampling reducing spatial dimensions of the input tensor while learning the most prominent features\n",
        "    MaxPooling2D(pool_size=CONFIGURATION[\"POOL_SIZE\"], strides=CONFIGURATION[\"N_STRIDES\"]*2),\n",
        "\n",
        "    # dropout, regularization technique to reduce overfitting, prevent model from relying too heavily on specific neurons by randomly \"droping out\"\n",
        "    Dropout(CONFIGURATION[\"DROPOUT_RATE\"]),\n",
        "\n",
        "    # Convolution layer 2\n",
        "    Conv2D(filters=CONFIGURATION[\"N_FILTERS\"]*2+4, kernel_size=CONFIGURATION[\"KERNEL_SIZE\"], strides=CONFIGURATION[\"N_STRIDES\"], padding=\"valid\", activation=\"relu\", kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "\n",
        "    # Batch normaliation\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # Downsampling\n",
        "    MaxPooling2D(pool_size=CONFIGURATION[\"POOL_SIZE\"], strides=CONFIGURATION[\"N_STRIDES\"]*2),\n",
        "\n",
        "    # Flatten, convert multi-dimensional input tensor into a one-dimensional tensor\n",
        "    Flatten(),\n",
        "\n",
        "    # Dense Layers for classification from extracted features\n",
        "    # Dense layer 1\n",
        "    Dense(CONFIGURATION[\"N_DENSE_1\"], activation=\"relu\", kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "    BatchNormalization(),\n",
        "    Dropout(CONFIGURATION[\"DROPOUT_RATE\"]),\n",
        "\n",
        "    # Dense layer 2\n",
        "    Dense(CONFIGURATION[\"N_DENSE_2\"], activation=\"relu\", kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # Dense layer 3\n",
        "    Dense(CONFIGURATION[\"N_DENSE_3\"], activation=\"softmax\"),\n",
        "])\n",
        "lenet_model.summary()"
      ],
      "metadata": {
        "id": "_vAL1LVIHCyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(lenet_model, to_file=\"model.png\", show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "Z7-WcwbKTG2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RESNET34 (Residual Network with 34 layers) MODEL"
      ],
      "metadata": {
        "id": "Kz6UYRbrN1B0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomConv2D(Layer):\n",
        "  def __init__(self, filters, kernel_size, strides):\n",
        "    super(CustomConv2D, self).__init__()\n",
        "    self.conv = Conv2D(filters, kernel_size=kernel_size, strides=strides, padding=\"same\", activation=\"relu\")\n",
        "    self.bn = BatchNormalization()\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.conv(inputs)\n",
        "    x = self.bn(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class ResidualBlock(Layer):\n",
        "  def __init__(self, filters, strides=1):\n",
        "    super(ResidualBlock, self).__init__(name=\"res_block\")\n",
        "    self.conv1 = CustomConv2D(filters, 3, strides)\n",
        "    self.conv2 = CustomConv2D(filters, 3, 1)\n",
        "\n",
        "    self.dotted = (strides!=1)\n",
        "    if self.dotted:\n",
        "      self.dimensionMatching = CustomConv2D(filters, 1, 2)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.conv2(x)\n",
        "\n",
        "    if self.dotted:\n",
        "      inputs = self.dimensionMatching(inputs)\n",
        "\n",
        "    x = Add()([x, inputs])\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class ResNet34(Model):\n",
        "  def __init__(self):\n",
        "    super(ResNet34, self).__init__(name = \"resnet34\")\n",
        "    self.conv = CustomConv2D(64, 7, 2)\n",
        "    self.max_pool = MaxPooling2D(pool_size=3, strides=2, padding=\"same\")\n",
        "\n",
        "    self.block1 = ResidualBlock(64)\n",
        "    self.block2 = ResidualBlock(64)\n",
        "    self.block3 = ResidualBlock(64)\n",
        "\n",
        "    self.block4 = ResidualBlock(128, 2)\n",
        "    self.block5 = ResidualBlock(128)\n",
        "    self.block6 = ResidualBlock(128)\n",
        "    self.block7 = ResidualBlock(128)\n",
        "\n",
        "    self.block8 = ResidualBlock(256, 2)\n",
        "    self.block9 = ResidualBlock(256)\n",
        "    self.block10 = ResidualBlock(256)\n",
        "    self.block11 = ResidualBlock(256)\n",
        "    self.block12 = ResidualBlock(256)\n",
        "    self.block13 = ResidualBlock(256)\n",
        "\n",
        "    self.block14 = ResidualBlock(512, 2)\n",
        "    self.block15 = ResidualBlock(512)\n",
        "    self.block16 = ResidualBlock(512)\n",
        "\n",
        "    self.avgpool = GlobalAveragePooling2D()\n",
        "\n",
        "    self.fc = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation=\"softmax\")\n",
        "\n",
        "  def call(self, inputs):\n",
        "\n",
        "    x = Resizing(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"])(inputs)\n",
        "    x = Rescaling(1./255)(x)\n",
        "\n",
        "    x = self.conv(x)\n",
        "    x = self.max_pool(x)\n",
        "\n",
        "    x = self.block1(x)\n",
        "    x = self.block2(x)\n",
        "    x = self.block3(x)\n",
        "\n",
        "    x = self.block4(x)\n",
        "    x = self.block5(x)\n",
        "    x = self.block6(x)\n",
        "    x = self.block7(x)\n",
        "\n",
        "    x = self.block8(x)\n",
        "    x = self.block9(x)\n",
        "    x = self.block10(x)\n",
        "    x = self.block11(x)\n",
        "    x = self.block12(x)\n",
        "    x = self.block13(x)\n",
        "\n",
        "    x = self.block14(x)\n",
        "    x = self.block15(x)\n",
        "    x = self.block16(x)\n",
        "\n",
        "    x = self.avgpool(x)\n",
        "\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "KJZJDXBQhi25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_34 = ResNet34()\n",
        "resnet_34(tf.zeros([1,224,224,3]))\n",
        "resnet_34.summary()"
      ],
      "metadata": {
        "id": "GOaTsZtgeGpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(resnet_34, to_file=\"model.png\", show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "gku_uAkTCl5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EFFICIENTNET MODEL"
      ],
      "metadata": {
        "id": "Lc9cbrE_dC_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRANSFER LEARNING"
      ],
      "metadata": {
        "id": "FZvbHxmRdJvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = tf.keras.applications.efficientnet.EfficientNetB4(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3),\n",
        ")\n",
        "# First Set False For Tansfer Learning then True For Fine Tunning\n",
        "feature_extractor.trainable = False"
      ],
      "metadata": {
        "id": "v7YtXs61dBvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = Input(shape=(None, None, 3))\n",
        "x = Resizing(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"])(input)\n",
        "x = Rescaling(1./255)(x)\n",
        "x = feature_extractor(x, training=False)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "output = Dense(3, activation='softmax')(x)\n",
        "\n",
        "efficient_net_model = Model(input, output)"
      ],
      "metadata": {
        "id": "XOpX27byec9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FINE TUNNING"
      ],
      "metadata": {
        "id": "ArrcH8iwllkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor_finetune = tf.keras.applications.efficientnet.EfficientNetB4(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3),\n",
        ")\n",
        "# First Set False For Tansfer Learning then True For Fine Tunning\n",
        "feature_extractor_finetune.trainable = False"
      ],
      "metadata": {
        "id": "ZMT9qwl3qAXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = Input(shape=(None, None, 3))\n",
        "\n",
        "x = Resizing(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"])(input)\n",
        "x = Rescaling(1./255)(x)\n",
        "\n",
        "x = feature_extractor(x, training=False)\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "x = Dense(CONFIGURATION[\"N_DENSE_1\"], activation=\"relu\", kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"]))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(CONFIGURATION[\"DROPOUT_RATE\"])(x)\n",
        "\n",
        "x = Dense(CONFIGURATION[\"N_DENSE_2\"], activation=\"relu\", kernel_regularizer=L2(CONFIGURATION[\"REGULARIZATION_RATE\"]))(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "output = Dense(CONFIGURATION[\"N_DENSE_3\"], activation=\"softmax\")(x)\n",
        "\n",
        "finetuned_model = Model(input, output)"
      ],
      "metadata": {
        "id": "X2g3fNw_qBQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING"
      ],
      "metadata": {
        "id": "wl53Xnb8jr47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CALLBACKS"
      ],
      "metadata": {
        "id": "RTkfagwLgDcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    \"best_weights\",\n",
        "    monitor=\"val_accuracy\",\n",
        "    mode=\"max\",\n",
        "    verbose=1,\n",
        "    save_best_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "rytjzJWcgFhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOSS FUNCTIO & METRICS"
      ],
      "metadata": {
        "id": "oWcZuPIOhj4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From logits = False, means that output vector will be a probability distribution having sum = 1\n",
        "# Else we can set it True if output is raw that means it is not a pd and it is directly take from neurons output\n",
        "loss_function = CategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "# Categorical accuracy  [0, 0, 1] matches with [0, 0.1, 0.9] (that is it will check whether highest value matches or not for same class)\n",
        "# Top K Categorical accuracy compares how often target in top k prediction\n",
        "metrics = [CategoricalAccuracy(name=\"accuracy\"), TopKCategoricalAccuracy(k=2, name=\"top_k_accuracy\")]"
      ],
      "metadata": {
        "id": "CJHe7Sc7jtzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sparse Categorical\n",
        "# When label mode for dataset is \"int\" that 0[1,0,0], 1[0,1,0], 2[0,0,1] type\n",
        "# sparse_loss_functin = SparseCategoricalCrossentropy()"
      ],
      "metadata": {
        "id": "-yI2vtT1m6gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LENET"
      ],
      "metadata": {
        "id": "rftSrpZDhPS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_model.compile(\n",
        "    optimizer=Adam(learning_rate=CONFIGURATION[\"LEARNING_RATE\"]),\n",
        "    loss=loss_function,\n",
        "    metrics=metrics\n",
        ")"
      ],
      "metadata": {
        "id": "E0IW9wFrj9ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = lenet_model.fit(\n",
        "    training_dataset,\n",
        "    validation_data = validation_dataset,\n",
        "    epochs=CONFIGURATION[\"N_EPOCHS\"],\n",
        "    verbose=1,\n",
        ")"
      ],
      "metadata": {
        "id": "1Vj6wqJspt83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RESNET_34"
      ],
      "metadata": {
        "id": "A3IZkhSQEZOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_34.compile(\n",
        "    optimizer=Adam(learning_rate=CONFIGURATION[\"LEARNING_RATE\"]),\n",
        "    loss=loss_function,\n",
        "    metrics=metrics\n",
        ")"
      ],
      "metadata": {
        "id": "phZjAtoXEeeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_resnet = resnet_34.fit(\n",
        "    training_dataset,\n",
        "    validation_data = validation_dataset,\n",
        "    epochs=CONFIGURATION[\"N_EPOCHS\"],\n",
        "    verbose=1,\n",
        ")"
      ],
      "metadata": {
        "id": "93uEyJ9FEqDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EFFICIENT NET"
      ],
      "metadata": {
        "id": "FryVTvmshThI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "efficient_net_model.compile(\n",
        "    optimizer=Adam(learning_rate=CONFIGURATION[\"LEARNING_RATE\"]),\n",
        "    loss=loss_function,\n",
        "    metrics=metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "Y75NmHWwrVq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G-PhaQSHVigu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_efficient_net = efficient_net_model.fit(training_dataset, validation_data=validation_dataset, epochs = 3, verbose=1)"
      ],
      "metadata": {
        "id": "oMLvE_2Zh_aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINETUNE"
      ],
      "metadata": {
        "id": "siYv0UTPrMzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model.compile(\n",
        "    optimizer=Adam(learning_rate=CONFIGURATION[\"LEARNING_RATE\"]),\n",
        "    loss=loss_function,\n",
        "    metrics=metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "m1sTWKJKrLLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_finetune = finetuned_model.fit(training_dataset, validation_data=validation_dataset, epochs = 10, verbose=1)"
      ],
      "metadata": {
        "id": "xbPiGLUNrpe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUATION"
      ],
      "metadata": {
        "id": "nuOR44oDvr7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_model.evaluate(validation_dataset)"
      ],
      "metadata": {
        "id": "USPqDEX2vtzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_34.evaluate(validation_dataset)"
      ],
      "metadata": {
        "id": "dOEAxNJtvyZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "efficient_net_model.evaluate(validation_dataset)"
      ],
      "metadata": {
        "id": "J5_cK3KTGVT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model.evaluate(validation_dataset)"
      ],
      "metadata": {
        "id": "sIV7fhNBIba3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VISUALIZING RESULTS OF TRAINING"
      ],
      "metadata": {
        "id": "vlUbatI1wQGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOSS"
      ],
      "metadata": {
        "id": "Vc58eU80wc9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LENET MODEL"
      ],
      "metadata": {
        "id": "1JYG1hQYIj06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.title(\"LOSS V/S EPOCH\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"Train\", \"Val\"])"
      ],
      "metadata": {
        "id": "RDksjkqbwUgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RESNET"
      ],
      "metadata": {
        "id": "5wv0IMO5I12a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_resnet.history[\"loss\"])\n",
        "plt.plot(history_resnet.history[\"val_loss\"])\n",
        "plt.title(\"LOSS V/S EPOCH\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"Train\", \"Val\"])"
      ],
      "metadata": {
        "id": "1AmQpODOIvJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EFFICIENT NET"
      ],
      "metadata": {
        "id": "dL7szUfXI6wO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_efficient_net.history[\"loss\"])\n",
        "plt.plot(history_efficient_net.history[\"val_loss\"])\n",
        "plt.title(\"LOSS V/S EPOCH\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"Train\", \"Val\"])"
      ],
      "metadata": {
        "id": "O2qpQtxNI-Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FINETUNED"
      ],
      "metadata": {
        "id": "Td5hZkHTJCYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.title(\"LOSS V/S EPOCH\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"Train\", \"Val\"])"
      ],
      "metadata": {
        "id": "CTSgBz8BJEXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ACCURACY"
      ],
      "metadata": {
        "id": "uzBx9s9CxJJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LENET MODEL"
      ],
      "metadata": {
        "id": "Bzc8y-cxIm1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"accuracy\"])\n",
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "plt.title(\"ACCURACY V/S EPOCH\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"Train\", \"Val\"])"
      ],
      "metadata": {
        "id": "hCafDYIrxLd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"top_k_accuracy\"])\n",
        "plt.plot(history.history[\"val_top_k_accuracy\"])\n",
        "plt.title(\"Top K Accuracy V/S EPOCH\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Topp K Accuracy\")\n",
        "plt.legend([\"Train\", \"Val\"])"
      ],
      "metadata": {
        "id": "tl4FKDF8xVeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING"
      ],
      "metadata": {
        "id": "al_0ArqFwNCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (14, 14))\n",
        "for images, labels in val_dataset.take(1):\n",
        "  for i in range(16):\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    plt.imshow(images[i]/255)\n",
        "    plt.title(\"Actual : \" + CLASS_NAMES[tf.argmax(labels[i]).numpy()] + \"\\nPredicted : \" + CLASS_NAMES[tf.argmax(lenet_model.predict(tf.expand_dims(images[i], axis=0)), axis=-1).numpy()[0]])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "qhRYgEp6wN-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HYCayXX90w5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PERFORMANCE"
      ],
      "metadata": {
        "id": "sN6adZqKjH4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CONFUSION MATRIX"
      ],
      "metadata": {
        "id": "NeXr1uz4mb9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels = []\n",
        "predicted_labels = []"
      ],
      "metadata": {
        "id": "Yh_cBJxBjJqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_images, batch_labels in validation_dataset:\n",
        "  true_labels.extend(tf.argmax(batch_labels, axis=-1).numpy().tolist())\n",
        "  predicted_labels.extend(tf.argmax(lenet_model.predict(batch_images), axis=-1).numpy().tolist())"
      ],
      "metadata": {
        "id": "Rezsb-CSm-g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(true_labels)\n",
        "print(len(true_labels))"
      ],
      "metadata": {
        "id": "bQGOcojKnQmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predicted_labels)\n",
        "print(len(predicted_labels))"
      ],
      "metadata": {
        "id": "Rb36kElspy-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "cm"
      ],
      "metadata": {
        "id": "_YWYsF6zrAvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", linewidths=2)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-V07-1tRuwQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qkU4ZaEPveS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VISUALISING INTERMEDIATE CONVOLUTION LAYERS"
      ],
      "metadata": {
        "id": "qSjBoRuMddwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_backbone = tf.keras.applications.vgg16.VGG16(\n",
        "    include_top = False,\n",
        "    weights = \"imagenet\",\n",
        "    input_shape = (CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3)\n",
        ")\n",
        "vgg_backbone.summary()"
      ],
      "metadata": {
        "id": "cr4YPCjXditg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_conv(layer_name):\n",
        "  if \"conv\" in layer_name:\n",
        "    return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "gaNS7CBtjat8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_maps = [layer.output for layer in vgg_backbone.layers[1:] if is_conv(layer.name)]"
      ],
      "metadata": {
        "id": "0PM9Ow82i59w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_map_model = Model(\n",
        "    inputs = vgg_backbone.input,\n",
        "    outputs = feature_maps\n",
        ")\n",
        "feature_map_model.summary()"
      ],
      "metadata": {
        "id": "uaSQnE9ejksV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(feature_maps))"
      ],
      "metadata": {
        "id": "kLnv32PmjvRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = cv2.imread(\"/content/dataset/Emotions Dataset/Emotions Dataset/test/happy/110020.jpg\")\n",
        "test_image = cv2.resize(test_image, (224, 224))\n",
        "\n",
        "im = tf.constant(test_image, dtype=tf.float32)\n",
        "im = tf.expand_dims(im, axis=0)\n",
        "\n",
        "f_maps = feature_map_model.predict(im)"
      ],
      "metadata": {
        "id": "R07whgU-kyku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(f_maps))"
      ],
      "metadata": {
        "id": "nop2idEtl2Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(f_maps)):\n",
        "  print(f_maps[i].shape)"
      ],
      "metadata": {
        "id": "it3HEfXPl94c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  plt.figure(figsize=(224, 224))\n",
        "  n_channels = f_maps[i].shape[3]\n",
        "  size = f_maps[i].shape[1]\n",
        "  joint_maps = np.ones((size, size*n_channels))\n",
        "\n",
        "  plt.subplot(3, 1, i+1)\n",
        "  for j in range(n_channels):\n",
        "    joint_maps[:, size*j:size*(j+1)] = f_maps[i][..., j]\n",
        "\n",
        "  plt.imshow(joint_maps[:, 0:448])\n",
        "  plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "lhnpawp9mIu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dnq8LGWLniYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VISION TRANSFORMERS"
      ],
      "metadata": {
        "id": "TxH7e8ZAvUi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = cv2.imread(\"/content/dataset/Emotions Dataset/Emotions Dataset/train/happy/110028.jpg\")"
      ],
      "metadata": {
        "id": "BlN2C9uWvW7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = cv2.resize(test_image, (256, 256))"
      ],
      "metadata": {
        "id": "mgg9hQEwvk-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_image)"
      ],
      "metadata": {
        "id": "paWMJkk8vwUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patches = tf.image.extract_patches(\n",
        "    images=tf.expand_dims(test_image, axis=0),\n",
        "    sizes=[1, 16, 16, 1],\n",
        "    strides=[1, 16, 16, 1],\n",
        "    rates=[1, 1, 1, 1],\n",
        "    padding=\"VALID\"\n",
        ")"
      ],
      "metadata": {
        "id": "1HVAT3ayv0qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patches.shape"
      ],
      "metadata": {
        "id": "_O6N0MUswi_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(patches.shape)\n",
        "patches = tf.reshape(patches, (1,256,768))\n",
        "print(patches.shape)"
      ],
      "metadata": {
        "id": "RBFT4WDYxP9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(patches.shape[1]):\n",
        "    plt.subplot(16, 16, i+1)\n",
        "    im = patches[0][i][j]\n",
        "    plt.imshow(tf.reshape(patches[0][i], (16,16,3)))\n",
        "    plt.axis(\"off\")\n"
      ],
      "metadata": {
        "id": "7tP0cyf6xSfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEncoder(Layer):\n",
        "  def __init__(self, N_PATCHES, HIDDEN_SIZE):\n",
        "    super(PatchEncoder, self).__init__(name=\"patch_encoder\")\n",
        "    self.linear_projection = Dense(HIDDEN_SIZE)\n",
        "    self.positional_embedding = Embedding()\n",
        "  def call(self, inputs):\n",
        "    patches = tf.image.extract_patches(\n",
        "        images=inputs,\n",
        "        sizes=[1, 16, 16, 1],\n",
        "        strides=[1, 16, 16, 1],\n",
        "        rates=[1, 1, 1, 1],\n",
        "        padding=\"VALID\"\n",
        "    )\n",
        "    patches = tf.reshape(patches, (patches.shape[0], -1, patches.shape[-1]))\n",
        "    ouput = self.linear_projection(patches)\n"
      ],
      "metadata": {
        "id": "bUPE7SN_xuig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIT WITH HUGGINGFACE"
      ],
      "metadata": {
        "id": "IXR2epowbN78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "CwMQ0X0xbRJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTModel, ViTConfig"
      ],
      "metadata": {
        "id": "FP9SmLlmbepP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configuration = ViTConfig()"
      ],
      "metadata": {
        "id": "WbmXnwQIuv12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViTModel(configuration)"
      ],
      "metadata": {
        "id": "0McXvSb6uzdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config)"
      ],
      "metadata": {
        "id": "D9DIQZqRu1WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTFeatureExtractor, TFViTModel"
      ],
      "metadata": {
        "id": "YGERhNeuu_ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resize_rescale_hf = tf.keras.Sequential([\n",
        "    Resizing(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
        "    Rescaling(1./255),\n",
        "    Permute((3,1,2))\n",
        "])"
      ],
      "metadata": {
        "id": "byoCn3BNxlzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = TFViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "inputs = Input(shape=(None,None,3))\n",
        "x = resize_rescale_hf(inputs)\n",
        "x = base_model.vit(x)[0][:,0,:]\n",
        "outputs = Dense(3, activation=\"softmax\")(x)\n",
        "hf_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n"
      ],
      "metadata": {
        "id": "9Gt8QGu3wE2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = cv2.imread(\"/content/dataset/Emotions Dataset/Emotions Dataset/train/happy/101883.jpg\")\n",
        "test_image = cv2.resize(test_image, (224, 224))"
      ],
      "metadata": {
        "id": "TnSGqe81wdPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_model(tf.expand_dims(test_image, axis=0))"
      ],
      "metadata": {
        "id": "POdzPg3u02pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_model.summary()"
      ],
      "metadata": {
        "id": "CgXLYYxh0-C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = CategoricalCrossentropy(from_logits=False)\n",
        "metrics = [CategoricalAccuracy(name=\"accuracy\"), TopKCategoricalAccuracy(k=2, name=\"top_k_accuracy\")]\n",
        "hf_model.compile(\n",
        "    optimizer=Adam(CONFIGURATION[\"LEARNING_RATE\"]),\n",
        "    loss=loss_function,\n",
        "    metrics=metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "2ldD9bYZ15WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogConfMatrix(Callback):\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "    for batch_images, batch_labels in validation_dataset:\n",
        "      true_labels.extend(tf.argmax(batch_labels, axis=-1).numpy().tolist())\n",
        "      predicted_labels.extend(tf.argmax(hf_model(batch_images), axis=-1).numpy().tolist())\n",
        "\n",
        "    cm = wandb.plot.confusion_matrix(\n",
        "        y_true = true_labels,\n",
        "        preds = predicted_labels,\n",
        "        class_names = CONFIGURATION[\"CLASS_NAMES\"]\n",
        "    )\n",
        "    wandb.log({\"conf_mat\": cm})\n"
      ],
      "metadata": {
        "id": "jOu-8G7281MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogResultsTable(Callback):\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    columns = [\"Image\", \"Predicted\", \"Label\"]\n",
        "    val_table = wandb.Table(columns=columns)\n",
        "    for batch_images, batch_labels in validation_dataset.take(1):\n",
        "      for image, label in zip(batch_images, batch_labels):\n",
        "        true_label = CONFIGURATION[\"CLASS_NAMES\"][tf.argmax(label).numpy()]\n",
        "        predicted_label = CONFIGURATION[\"CLASS_NAMES\"][tf.argmax(hf_model(tf.expand_dims(image, axis=0)), axis=-1).numpy()[0]]\n",
        "        row = [wandb.Image(image), predicted_label, true_label]\n",
        "        val_table.add_data(*row)\n",
        "    wandb.log({\"Model Results\": val_table})"
      ],
      "metadata": {
        "id": "G-Tmuxvq_tf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_hf_model = hf_model.fit(\n",
        "    training_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs = 3,\n",
        "    verbose=1,\n",
        "    callbacks=[WandbCallback(), LogConfMatrix(), LogResultsTable(), LogResultsTable()]\n",
        "    )"
      ],
      "metadata": {
        "id": "xGT3skdo2-hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_model.evaluate(validation_dataset)"
      ],
      "metadata": {
        "id": "jS5p3cBkXPWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOSS"
      ],
      "metadata": {
        "id": "ZcavPqs-VwWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_hf_model.history[\"loss\"])\n",
        "plt.plot(history_hf_model.history[\"val_loss\"])\n",
        "plt.title(\"LOSS V/S EPOCH\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"Train\", \"Val\"])"
      ],
      "metadata": {
        "id": "4IuDufKw3zii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ACCURACY"
      ],
      "metadata": {
        "id": "7kP5AyF1XMOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_hf_model.history[\"accuracy\"])\n",
        "plt.plot(history_hf_model.history[\"val_accuracy\"])\n",
        "plt.title(\"LOSS V/S EPOCH\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"Train\", \"Val\"])"
      ],
      "metadata": {
        "id": "KHReSNHcVzS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_model.evaluate(validation_dataset)"
      ],
      "metadata": {
        "id": "TkVeJwjmV98Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving And Loading Model (Google Drive)"
      ],
      "metadata": {
        "id": "41wfNutsmNAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_model.save(\"vit_finetuned\")"
      ],
      "metadata": {
        "id": "xkcw9kjHrcH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OOnLUueTxY6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/MyViTModel/ /content/vit_finetuned/"
      ],
      "metadata": {
        "id": "y09fyQ5cxH0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_vit_model = tf.keras.models.load_model(\"/content/vit_finetuned\")"
      ],
      "metadata": {
        "id": "xJipXKwt1PPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_vit_model.evaluate(validation_dataset)"
      ],
      "metadata": {
        "id": "DwaznEZe1gMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exporint To ONNX Format"
      ],
      "metadata": {
        "id": "Vc1P9tFhrj8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "NlbPoniMr1nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tf2onnx\n",
        "!pip install onnxruntime"
      ],
      "metadata": {
        "id": "uJDZ9lBFryLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversion From tensorflow to onnx"
      ],
      "metadata": {
        "id": "qP2DW0FJnSa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m tf2onnx.convert --saved-model vit_finetuned/ --output vit_onnx.onnx"
      ],
      "metadata": {
        "id": "WMEZtkBFty8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/vit_onnx.onnx /content/vit_onnx.onnx"
      ],
      "metadata": {
        "id": "i2Z-OGtCxyL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting Using Onnx Model"
      ],
      "metadata": {
        "id": "Ik8_TH8Zn2N_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as rt"
      ],
      "metadata": {
        "id": "AdoQumJDzv9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "id": "AJ9LK4GXhPyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "model = onnx.load(\"/content/vit_onnx.onnx\")"
      ],
      "metadata": {
        "id": "2sA57qHvhdZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "session = ort.InferenceSession(\"/content/vit_onnx.onnx\")"
      ],
      "metadata": {
        "id": "Fe6xsPg1ho2U"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_name = session.get_inputs()[0].name\n",
        "output_name = session.get_outputs()[0].name"
      ],
      "metadata": {
        "id": "ypt6S-ad6VPw"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = cv2.imread(\"/content/111073.jpg\")\n",
        "input_data = cv2.resize(input_data, (224, 224))\n",
        "input_data = tf.expand_dims(tf.cast(input_data, tf.float32), axis=0).numpy()"
      ],
      "metadata": {
        "id": "PYMm1NFZh48K"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = session.run([output_name], {input_name: input_data})"
      ],
      "metadata": {
        "id": "OcPcVL1eh1tM"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAlPey6Sh1MO",
        "outputId": "41850100-478c-4f0b-fe71-5b5dd33c0ab5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[1.7842471e-04, 9.9974543e-01, 7.6104137e-05]], dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-lwYYUmZjpxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Web Interface Using Gradio"
      ],
      "metadata": {
        "id": "RptuCozqnnVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNg3bsZF1zgL",
        "outputId": "b4a2abd4-e4c3-4b9d-c53a-8d779292f90e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-3.35.2-py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles (from gradio)\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from gradio) (3.8.4)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.2.7 (from gradio)\n",
            "  Downloading gradio_client-0.2.7-py3-none-any.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m288.4/288.4 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.14.0 (from gradio)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.0.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gradio) (1.22.4)\n",
            "Collecting orjson (from gradio)\n",
            "  Downloading orjson-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio) (8.4.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from gradio) (1.10.9)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: pygments>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.14.0)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gradio) (2.27.1)\n",
            "Collecting semantic-version (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.0 (from gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (2023.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (23.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.12.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.65.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio)\n",
            "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2022.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.3)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (2023.5.7)\n",
            "Collecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio)\n",
            "  Downloading httpcore-0.17.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (3.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gradio) (1.26.16)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.7.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio)\n",
            "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->gradio) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio) (1.1.1)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4694 sha256=3e905974d39e327f57176ae0bea2c32dbaa286868f1d54e46d3cd25787ffb233\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/c2/0e/3b9c6845c6a4e35beb90910cc70d9ac9ab5d47402bd62af0df\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uc-micro-py, semantic-version, python-multipart, orjson, markdown-it-py, h11, aiofiles, uvicorn, starlette, mdit-py-plugins, linkify-it-py, huggingface-hub, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "Successfully installed aiofiles-23.1.0 fastapi-0.99.1 ffmpy-0.3.0 gradio-3.35.2 gradio-client-0.2.7 h11-0.14.0 httpcore-0.17.2 httpx-0.24.1 huggingface-hub-0.15.1 linkify-it-py-2.0.2 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 orjson-3.9.1 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.27.0 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "GPrmf2FP442Q"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx\n",
        "!pip install onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H89cYy_qomXd",
        "outputId": "36042164-6add-4e36-e940-25c4d32f640a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.14.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.22.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.6.3)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (1.15.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (2.0.7)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.11.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as rt"
      ],
      "metadata": {
        "id": "qu5sSV-romXc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "model = onnx.load(\"/content/vit_onnx.onnx\")"
      ],
      "metadata": {
        "id": "4D72H2nxomXd"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "session = ort.InferenceSession(\"/content/vit_onnx.onnx\")"
      ],
      "metadata": {
        "id": "C8gJc95AomXd"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_name = session.get_inputs()[0].name\n",
        "output_name = session.get_outputs()[0].name"
      ],
      "metadata": {
        "id": "fwjYfqeromXe"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLASS_NAMES = [\"Angry\", \"Happy\", \"Sad\"]\n",
        "def predict_image(im):\n",
        "  im = tf.expand_dims(tf.cast(im, tf.float32), axis=0).numpy()\n",
        "  prediction = session.run([output_name], {input_name: im})\n",
        "  return {CLASS_NAMES[i]: float(prediction[0][0][i]) for i in range(3)}"
      ],
      "metadata": {
        "id": "qxvckyCe48Tn"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = gr.inputs.Image(shape=(224, 224))\n",
        "label = gr.outputs.Label(num_top_classes=3)\n",
        "iface = gr.Interface(fn=predict_image, inputs=image, outputs=label, capture_session=True)\n",
        "iface.launch(debug=\"True\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "9q9wIMqT6CXn",
        "outputId": "62f4421a-85c1-40f0-b48e-b40a50a755e7"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/inputs.py:259: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/inputs.py:262: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  super().__init__(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/outputs.py:197: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/outputs.py:200: UserWarning: The 'type' parameter has been deprecated. Use the Number component instead.\n",
            "  super().__init__(num_top_classes=num_top_classes, type=type, label=label)\n",
            "<ipython-input-71-f665dea269bc>:3: UserWarning: `capture_session` parameter is deprecated, and it has no effect\n",
            "  iface = gr.Interface(fn=predict_image, inputs=image, outputs=label, capture_session=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U-VGCcb8pBzP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}